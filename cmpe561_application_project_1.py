# -*- coding: utf-8 -*-
"""CMPE561 Application Project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ON8fiBb2XN5lrqs2xEqVi8UUDQbsa7sy

This project is for CMPE561 (Natural Language Processing) Master's course in Boğaziçi University.
It aims to build a rule-based Turkish text normalization and preprocessing pipeline. The system is
designed to handle noisy user input and prepare it for downstream NLP tasks such as parsing or
information extraction.

The project consists of several components:

1. Rule-Based Sentence Splitter: A custom sentence splitter is implemented using heuristics and
abbreviation lists to handle edge cases in Turkish punctuation and sentence boundaries.

2. Rule-Based Tokenizer: A robust tokenizer is created to handle Turkish clitics, punctuation,
multi-word expressions (MWEs), and various non-standard entities like hashtags, mentions, emails,
links, and currency formats. The tokenizer supports external lexicon files for MWEs and suffixes.

3. Morphological Stemmer: Two versions of a Turkish stemmer are developed. Both use a list of suffixes
and protected words to strip inflectional and some derivational suffixes. Stemmer2 improves on Stemmer1
by sorting suffixes by length and applying them iteratively until no more can be removed.

4. Pattern Extractors: Specialized regex-based extractors are implemented to detect unnormalized elements
in text such as dates, numbers, currency formats, hashtags, emails, and links. These are preserved during
normalization.

5. Spelling Correction: The system includes a hybrid spell corrector. It first checks a curated list of
common corrections (`corrected_trspell10.csv`), and if not found, finds the nearest word from a Turkish
word list based on Levenshtein distance.

6. Normalizer: A full pipeline that integrates lowercasing, pattern preservation, punctuation removal,
number-to-word conversion (using `num2words`), and spelling correction to output a normalized version
of noisy Turkish text.

7. Static and Dynamic Stopword Removal: Static stopword removal uses a predefined list, while dynamic
stopword removal is based on frequency counts from the input corpus (removing words that appear more
than 20 times).

The pipeline is modular, extensible, and capable of processing real-world Turkish text, particularly
useful in social media, OCR outputs, or conversational data. The focus is on rule-based methods that
do not rely on large-scale pretrained models, making the system interpretable and lightweight.
"
"""

# Open and read the content of the text file
f = open("DemoText.txt")
corpus = f.read()

# ---- SENTENCE SPLITTER RULE BASED ----

import re

# Load a set of Turkish abbreviations from a file
def load_abbreviations(filename):
    with open(filename, "r", encoding="utf-8") as file:
        return set(line.strip() for line in file)  # each line is one abbreviation

# Load the abbreviation list into memory
turkish_abbreviations = load_abbreviations("turkish_abbreviations.txt")

# Check if a word is in the abbreviation list
def is_abbreviation(word):
    return word in turkish_abbreviations

# Rule-based sentence splitter function
def split_sentences(text):
    sentences = []            # to hold final sentences
    current_sentence = []     # to accumulate words for the current sentence
    buffer = ""               # to collect characters one by one
    i = 0                     # current position in the text

    while i < len(text):
        char = text[i]
        buffer += char        # add character to buffer

        if char == " ":
            # Word completed; move it to current_sentence if buffer is non-empty
            if buffer.strip():
                current_sentence.append(buffer.strip())
            buffer = ""

        elif char in {".", "!", "?"}:
            word = buffer.strip()

            if char == "." and is_abbreviation(word):
                # If the word is an abbreviation, don't split the sentence
                current_sentence.append(buffer.strip())
                buffer = ""
            else:
                # Sentence end punctuation not part of an abbreviation
                current_sentence.append(buffer.strip())
                buffer = ""

                # Lookahead to next character to decide if this is sentence boundary
                next_char = text[i + 1] if i + 1 < len(text) else ""

                # Check for common sentence boundary conditions
                if next_char in {" ", "\n"} or next_char.isupper() or next_char == "":
                    # Join current sentence tokens and add to list
                    sentences.append(" ".join(current_sentence).strip())
                    current_sentence = []

        i += 1

    # Catch any remaining buffer or sentence
    if current_sentence or buffer.strip():
        if buffer.strip():
            current_sentence.append(buffer.strip())
        sentences.append(" ".join(current_sentence).strip())

    return sentences

# Example usage with a test input
# Slice part of the corpus and append a test sentence to evaluate behavior
text = corpus[:552] + "Dr. Eylül Tekinoğlu randevularını kapatmış, acile geçmiş."
print(text + "\n")

# Split the text into sentences using the rule-based function
sentences = split_sentences(text)

# Print each sentence with index
for idx, sentence in enumerate(sentences, 1):
    print(f"Sentence {idx}: {sentence}")

# ---- TOKENIZER RULE BASED ----

# Import required modules
import re
from nltk.tokenize import MWETokenizer

# Define the rule-based tokenizer class
class RuleBasedTokenizer:
    def __init__(self, clitics_lexicon=None, abbreviations_lexicon=None, mwe_lexicon_file=None):
        # Set default Turkish clitics if none are provided
        self.clitics_lexicon = clitics_lexicon or ["'de", "'da", "'te", "'ta", "'ki", "'den", "'dan"]
        # Set default abbreviations
        self.abbreviations_lexicon = abbreviations_lexicon or ["Dr.", "Prof.", "Sn.", "Av.", "vb.", "vs.", "m²", "Mah.", "Sk.", "Doç."]

        # Initialize MWE patterns list
        self.mwe_patterns = []
        if mwe_lexicon_file:
            self.load_mwe_lexicon(mwe_lexicon_file)

        # Regular expressions for special patterns like emails, dates, hashtags, links, currency, etc.
        self.special_cases = [
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b',  # email addresses
            r'\b(\d{1,2}[.\/-]\d{1,2}[.\/-]\d{4})\b',                # date formats (e.g., 01/01/2023)
            r'^[0-9]{1,2}[\.: ][0-9]{2}#',                          # time expressions (e.g., 12:30)
            r'#\w+',                                                # hashtags
            r'\b((http(s)?:\/\/)?www\.[a-zA-Z0-9]+\.[a-zA-Z]{2,}(\.[a-zA-Z]{2,})?([\/\w\-.]*)?)\b',  # URLs
            r'^@\w+\b',                                             # mentions (e.g., @user)
            r'(\b\d{1,3}(\.\d{3})*(,\d+)?[₺$€£]|[₺$€£]\d{1,3}(\.\d{3})*(,\d+)?\b)'  # currency formats
        ]

    # Load multi-word expressions (MWEs) from a file and compile them into regex patterns
    def load_mwe_lexicon(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                words = line.strip().split()
                if words:
                    # Compile a regex to match the MWE with optional suffixes
                    pattern = r'\b' + r'\s+'.join(words[:-1]) + rf'\s+{words[-1]}(\w*)\b'
                    self.mwe_patterns.append(re.compile(pattern, re.IGNORECASE))

    # Main tokenize function
    def tokenize(self, input_sentence, split_token='<*>'):
        sentence = input_sentence

        # Highlight special cases by adding spaces around them
        for pattern in self.special_cases:
            sentence = re.sub(pattern, r' \g<0> ', sentence)

        # Merge multi-word expressions using split_token
        for mwe_pattern in self.mwe_patterns:
            sentence = mwe_pattern.sub(lambda m: m.group(0).replace(" ", split_token), sentence)

        # Add whitespace around unambiguous punctuation
        sentence = re.sub(r'([?!()\[\]{};])', r' \1 ', sentence)

        # Add space around commas that aren't part of numbers
        sentence = re.sub(r'(?<!\d),(?!\d)', r' ,

# Clone the Universal Dependencies Turkish-BOUN corpus from GitHub
!git clone https://github.com/UniversalDependencies/UD_Turkish-BOUN.git

# Install the conllu package to read CoNLL-U formatted files
!pip install conllu

# ---- Extracting Only the Text from the UD Corpus ----

from conllu import parse  # Import the parser for CoNLL-U files

# Open and read the test file from the corpus
with open("UD_Turkish-BOUN/tr_boun-ud-test.conllu", "r", encoding="utf-8") as f:
    data = parse(f.read())  # Parse the CoNLL-U content into structured sentences

# Extract raw sentence text by joining the 'form' field (actual word) of each token
corpus_sentences = [" ".join([token['form'] for token in sentence]) for sentence in data]

# Print the list of reconstructed sentences
print(corpus_sentences)

# Stemmer1

def turkce_stemmer_1(kelime):
    # Manually added words that should be protected from stemming
    ek_kelimeler=["elma","saksı"]

    # Load protected words (those whose suffixes shouldn't be removed)
    with open("protected_words.txt", 'r', encoding='utf-8') as protected_words:
        korunan_kelimeler = [satir.strip() for satir in protected_words if satir.strip()]
        # Add any additional protected words not already in the list
        for ek_kelime in ek_kelimeler:
            if ek_kelime not in korunan_kelimeler:
                korunan_kelimeler.append(ek_kelime)

    # Placeholder for manually added suffixes (not used in this example)
    ek_ekler=[]

    # Load suffix list from file
    with open("suffix.txt", 'r', encoding='utf-8') as suffixes:
        ekler = [satir.strip() for satir in suffixes if satir.strip()]
        # Add any additional suffixes not already in the list
        for ek_ek in ek_ekler:
            if ek_ek not in ekler:
                ekler.append(ek_ek)

    muhtemel_ekler = []  # to hold all suffix candidates found in the word
    kelime_uzunluk = len(kelime)

    # Iterate through all substrings of the word to find known suffixes
    for i in range(kelime_uzunluk - 1, -1, -1):
        for j in range(i, kelime_uzunluk):
            muhtemel_ek = kelime[i:j+1]
            if muhtemel_ek in ekler:
                muhtemel_ekler.append(muhtemel_ek)

    # Attempt to remove detected suffixes from the word
    kok = kelime
    for ek in muhtemel_ekler:
        if kok in korunan_kelimeler:
            kok = kok  # skip modification
            break
        elif kok.endswith(ek):
            kok = kok[:-len(ek)]  # remove suffix

    # Output found root
    print("Stemmer1'in bulduğu kök:", kok)
    return kok, muhtemel_ekler

# Example word to stem
kelime = "çiçekçininkiler"
kok, muhtemel_ekler = turkce_stemmer_1(kelime)

# Print the final root
print("Kök:", kok)

# Optionally print sorted suffixes (by length, descending)
# print("Muhtemel Ekler:", sorted(set(muhtemel_ekler), key=len, reverse=True))

# Stemmer2

"""
Key Differences from Stemmer1:

- Suffixes are sorted by length before removal → helps ensure longer (more specific) suffixes are prioritized.

- Uses a while loop with re-evaluation after each removal → allows for multi-step stripping.

- Avoids over-stemming by rechecking protection list after each step.
"""


def turkce_stemmer_2(kelime):
    # Manually added protected root words that should not be stemmed
    ek_kelimeler = ["elma", "saksı"]

    # Load protected words from file
    with open("protected_words.txt", 'r', encoding='utf-8') as protected_words:
        korunan_kelimeler = [satir.strip() for satir in protected_words if satir.strip()]
        # Add manual ones if they’re not already in the file
        for ek_kelime in ek_kelimeler:
            if ek_kelime not in korunan_kelimeler:
                korunan_kelimeler.append(ek_kelime)

    # Placeholder for additional suffixes (if any)
    ek_ekler = []

    # Load suffix list from file
    with open("suffix.txt", 'r', encoding='utf-8') as suffixes:
        ekler = [satir.strip() for satir in suffixes if satir.strip()]
        # Append manually specified suffixes
        for ek_ek in ek_ekler:
            if ek_ek not in ekler:
                ekler.append(ek_ek)

    muhtemel_ekler = []  # List to store suffix candidates
    kelime_uzunluk = len(kelime)

    # Find all substrings at the end of the word that match known suffixes
    for i in range(kelime_uzunluk - 1, -1, -1):
        for j in range(i, kelime_uzunluk):
            muhtemel_ek = kelime[i:j+1]
            # Only keep suffixes of length > 1 that are in suffix list
            if muhtemel_ek in ekler and len(muhtemel_ek) > 1:
                muhtemel_ekler.append(muhtemel_ek)

    # Remove duplicates and sort suffixes by length (longest first)
    muhtemel_ekler = sorted(set(muhtemel_ekler), key=len, reverse=True)

    kok = kelime  # Start with full word
    ek_index = 0  # Index to iterate over possible suffixes

    # Loop to remove suffixes iteratively from longest to shortest
    while ek_index < len(muhtemel_ekler):
        ek = muhtemel_ekler[ek_index]

        # If the current word is protected, stop stemming
        if kok in korunan_kelimeler:
            break

        # If the word ends with a known suffix, remove it
        elif kok.endswith(ek):
            kok = kok[:-len(ek)]
            ek_index = 0  # Restart checking from longest suffix again
        else:
            ek_index += 1  # Try next suffix

    # Output result
    print("Stemmer2'nin bulduğu kök:", kok)
    return kok, muhtemel_ekler


# Example usage
kelime = "üstümdekiler"
kok, muhtemel_ekler = turkce_stemmer_2(kelime)

# Print final root
print("Kök:", kok)
# Print sorted list of possible suffixes found
print("Muhtemel Ekler:", sorted(set(muhtemel_ekler), key=len, reverse=True))

# Concatenate all sentences into a single string
corpus_sentences = corpus

test_sentence = ""
for sentence in corpus_sentences:
    test_sentence += " "
    test_sentence += sentence

# Initialize the tokenizer with the Turkish MWE file
mwe_lexicon_file = "turkish_mwe.txt"
tokenizer = RuleBasedTokenizer(mwe_lexicon_file=mwe_lexicon_file)

# Tokenize the entire test text
tokens = tokenizer.tokenize(test_sentence)

# Iterate through all tokens and apply both stemmers
for i in range(len(tokens)):
    print("Kelime", tokens[i])  # Print current word
    kok, muhtemel_ekler = turkce_stemmer_1(tokens[i])  # Apply stemmer 1
    kok, muhtemel_ekler = turkce_stemmer_2(tokens[i])  # Apply stemmer 2
    print()

# Example corpus (in English) for pattern matching tests
import re

corpus = """For those who follow social media transitions on Capitol Hill, this will be a little different. “While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman, #kori, wrote an e-mail to monica@com.tr in a blog post Monday. But in a break from his past (his old past) rhetoric about curtailing immigration, the 5. GOP nominee proclaimed that as president he would allow “tremendous numbers” of legal immigrants based on a “merit system”! The new spending is fueled by Clinton’s large bank account? What she’s saying and what she’s doing, it — actually, it’s unbelievable… $5,000 per person, the maximum allowed. By comparison, it costs $103.7 million to build the NoMa infill Metro station (https:\\nomams.org), which opened in 2004."""

# Function to find unnormalized patterns such as dates, emails, tags, links, etc.
def find_cases_DONTnormalize(corpus):
    # Regex to find dates (e.g. 01/01/2023)
    date_pattern = r"\b(\d{1,2}[.\/-]\d{1,2}[.\/-]\d{4})\b"
    dates = re.findall(date_pattern, corpus)

    # Regex to find email addresses
    mail_pattern = r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b"
    mails = re.findall(mail_pattern, corpus)

    # Regex to find time expressions (e.g. 12:30 or 14.45)
    time_pattern = r"\b[0-9]{1,2}[\.: ][0-9]{2}\b"
    times = re.findall(time_pattern, corpus)

    # Regex to find hashtags (e.g. #example)
    tag_pattern = r"#\w+"
    tags = re.findall(tag_pattern, corpus)

    # Regex to find links (e.g. www.example.com)
    link_pattern = r"\b((http(s)?:\/\/)?www\.[a-zA-Z0-9]+\_

!pip install num2words

import pandas as pd

# Load the spelling correction dictionary (input: incorrect word, gold: correct word)
df_corrections = pd.read_csv('corrected_trspell10.csv', encoding='latin1')
corrections_dict = dict(zip(df_corrections['input'], df_corrections['gold']))

# Load the cleaned Turkish lexicon (frequent words)
with open('10000_frequent_turkish_word.txt', 'r', encoding='latin1') as file:
    cleaned_corpus = [line.strip() for line in file.readlines()]

# Initialize the rule-based tokenizer
tokenizer = RuleBasedTokenizer()

# Levenshtein distance implementation to measure how close two words are
def levenshteinDistance(s1, s2):
    if len(s1) > len(s2):
        s1, s2 = s2, s1  # Ensure s1 is the shorter string

    distances = range(len(s1) + 1)  # Initial distances for an empty s2 prefix
    for i2, c2 in enumerate(s2):
        distances_ = [i2 + 1]  # Start new row
        for i1, c1 in enumerate(s1):
            if c1 == c2:
                distances_.append(distances[i1])  # No cost
            else:
                # Minimum cost of insert, delete, or substitute
                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))
        distances = distances_
    return distances[-1]

# Return the closest matching word in the lexicon to the input word
def get_closest_word(word, lexicon, max_distance=2):
    min_distance = float('inf')  # Start with a high distance
    closest_word = word  # Default is the word itself
    for lex_word in lexicon:
        distance = levenshteinDistance(word, lex_word)
        if distance < min_distance and distance <= max_distance:
            min_distance = distance
            closest_word = lex_word  # Update closest match
    return closest_word

# Spell correction function for an input sentence
def correct_sentence(sentence):
    # Tokenize the input sentence
    tokens = tokenizer.tokenize(sentence)

    corrected_tokens = []

    for token in tokens:
        if token in corrections_dict:
            # If token exists in dictionary of corrections, use the correct version
            corrected_tokens.append(str(corrections_dict[token]))
        else:
            # Otherwise, use closest match from the lexicon (if within distance)
            closest_word = get_closest_word(token, list(cleaned_corpus))
            corrected_tokens.append(str(closest_word))

    # Reconstruct the sentence
    corrected_sentence = ' '.join(corrected_tokens)
    return corrected_sentence

# Example sentence with typos
input_sentence = "br calısmada öldu bilmiyorm kiii"

# Get the corrected version
normalized_sentence = correct_sentence(input_sentence)

# Output the result
print("Corrected Sentence:", normalized_sentence)

from num2words import num2words

# Define the normalization function
def normalizer(corpus):
    # Step 1: Convert the entire corpus to lowercase
    corpus = corpus.lower()

    # Step 2: Identify special cases (to preserve), numbers, and punctuation
    cases_DONTnormalize = find_cases_DONTnormalize(corpus)
    print(cases_DONTnormalize)  # Debug output: what will NOT be normalized

    numbers = find_number(corpus)
    # print(numbers)  # Debug if needed

    punctuations = find_punctuation(corpus)
    # print(punctuations)  # Debug if needed

    # Step 3: Tokenize the corpus using rule-based tokenizer
    tokens_normalizer = tokenizer.tokenize(corpus)
    corpus_normalized = ""

    # Step 4: Loop through each token for conditional normalization
    for token in tokens_normalizer:
        print(token)  # Debug each token

        if token in cases_DONTnormalize:
            # If token is in the special cases list (e.g., email, hashtag), keep it as is
            corpus_normalized += " " + token

        elif token in numbers:
            # If token is a number, convert to its written form in Turkish
            token_digit = int(token)
            token = num2words(token_digit, lang="tr")
            corpus_normalized += " " + token

        else:
            # Remove punctuation characters from the token
            for i in token:
                if i in punctuations:
                    token = token.replace(i, "")

            corpus_normalized += " " + token

    # Step 5: Apply spelling correction on the fully normalized sentence
    corpus_normalized = correct_sentence(corpus_normalized)

    return corpus_normalized

# Example input with various special tokens, numbers, and punctuation
test_sentence_normalizer = """For those who follow social media transitions on Capitol Hill, this will be a little different. “While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman, #kori, wrote an e-mail to monica@com.tr in a blog post Monday. But in a break from his past (his old past) rhetoric about curtailing immigration, the 5. GOP nominee proclaimed that as president he would allow “tremendous numbers” of legal immigrants based on a “merit system”! The new spending is fueled by Clinton’s large bank account? What she’s saying and what she’s doing, it — actually, it’s unbelievable… $5,000 per person, the maximum allowed. By comparison, it costs $103.7 million to build the NoMa infill Metro station (https:\\nomams.org), which opened in 2004."""

# Run the normalization
print(normalizer(test_sentence_normalizer))

import re

# Load stopwords from a file, removing line numbers if present
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        stopwords = set()
        for line in file:
            line = line.strip()  # Remove leading/trailing whitespace
            if line:
                cleaned_line = re.sub(r'^\d+\s+', '', line)  # Remove numbers at the start (e.g., "1 ve" → "ve")
                stopwords.add(cleaned_line.lower())  # Convert to lowercase and add to set
    return stopwords

# Function to remove static stopwords from a given text
def static_stopword_remover(text, stopwords):
    tokenizer = RuleBasedTokenizer()  # Use your custom tokenizer
    words = tokenizer.tokenize(text)  # Tokenize the input text
    non_stopwords = []

    for word in words:
        if word.lower() not in stopwords:  # Check word in lowercase form
            non_stopwords.append(word)  # Keep non-stopword

    # Optionally print tokenized and filtered words
    # print("Tokenized Words:", words)
    # print("Stopwords:", stopwords)

    return " ".join(non_stopwords)  # Reconstruct and return filtered sentence

# Main execution block
if __name__ == "__main__":
    stopwords_file = "turkish_stopwords.txt"  # Path to stopwords file
    stopwords = load_stopwords(stopwords_file)  # Load stopword list

    # Sample input text
    sample_text = "For those who follow social media transitions on Capitol Hill, this will be a little different. “While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman, #kori, wrote an e-mail to monica@com.tr in a blog post Monday. But in a break from his past (his old past) rhetoric about curtailing immigration, the 5. GOP nominee proclaimed that as president he would allow “tremendous numbers” of legal immigrants based on a “merit system”! The new spending is fueled by Clinton’s large bank account? What she’s saying and what she’s doing, it — actually, it’s unbelievable… $5,000 per person, the maximum allowed. By comparison, it costs $103.7 million to build the NoMa infill Metro station (https:\\nomams.org), which opened in 2004."

    # Apply static stopword removal
    filtered_text = static_stopword_remover(sample_text, stopwords)

    # Print the results
    print("Original Text:", sample_text)
    print("Filtered Text:", filtered_text)

import pandas as pd

# Create a DataFrame from the list of tokens (assumed previously created)
df = pd.DataFrame(tokens, columns=['Kelime'])

# Count how many times each word appears in the corpus
frekans = df['Kelime'].value_counts()

# Filter the frequency list to keep only words that appear more than 20 times
frekans_20den_buyuk = frekans[frekans > 20]

# Convert the index (words) of the frequency series to a list
stopword_listesi = frekans_20den_buyuk.index.tolist()

# Print the dynamically generated stopword list
print(stopword_listesi)

# Function to remove dynamic stopwords based on frequency from a given text
def dynamic_stopword_remover(text, stopword_listesi):
    tokenizer = RuleBasedTokenizer()  # Use your custom tokenizer
    words = tokenizer.tokenize(text)  # Tokenize input text
    non_stopwords = []

    for word in words:
        if word.lower() not in stopword_listesi:  # Case-insensitive comparison
            non_stopwords.append(word)  # Keep only non-stopwords

    return " ".join(non_stopwords)  # Reconstruct cleaned sentence

# Sample input for testing
sample_text2 = "BFor those who follow social media transitions on Capitol Hill, this will be a little different. “While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman, #kori, wrote an e-mail to monica@com.tr in a blog post Monday. But in a break from his past (his old past) rhetoric about curtailing immigration, the 5. GOP nominee proclaimed that as president he would allow “tremendous numbers” of legal immigrants based on a “merit system”! The new spending is fueled by Clinton’s large bank account? What she’s saying and what she’s doing, it — actually, it’s unbelievable… $5,000 per person, the maximum allowed. By comparison, it costs $103.7 million to build the NoMa infill Metro station (https:\\nomams.org), which opened in 2004."

# Apply the dynamic stopword remover
filtered_text2 = dynamic_stopword_remover(sample_text2, stopword_listesi)

# Print the result
print("Original Text:", sample_text2)
print("Filtered Text:", filtered_text2)